import os
import streamlit as st
import warnings
from sentence_transformers import SentenceTransformer
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import faiss
import numpy as np

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="í™”í•™ ë¬¼ì§ˆ í™”ì¬ ì‚¬ê³  ì „ë¬¸ ì±—ë´‡", page_icon="ğŸ¤–")
st.title("í™”í•™ ë¬¼ì§ˆ í™”ì¬ ì‚¬ê³  ì „ë¬¸ ì±—ë´‡")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
@st.cache_resource
def load_documents(directory):
    documents = []
    filenames = []
    for filename in os.listdir(directory):
        if filename.endswith(".txt"):
            filepath = os.path.join(directory, filename)
            with open(filepath, 'r', encoding='utf-8') as file:
                text = file.read()
                documents.append(text)
                filenames.append(filename)
    return documents, filenames

# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
@st.cache_resource
def load_vector_store():
    # Hugging Face Sentence Transformer ëª¨ë¸ì„ ì‚¬ìš©í•œ ì„ë² ë”©
    embeddings = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

    # ì¸í’‹ íŒŒì¼ ë¡œë“œ
    documents, filenames = load_documents("/Users/macbook/Desktop/streamlit")

    # ë¬¸ì„œ ì„ë² ë”© ìƒì„±
    document_embeddings = embeddings.encode(documents)  # í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©
    
    # FAISS ì¸ë±ìŠ¤ ìƒì„±
    index = faiss.IndexFlatL2(document_embeddings.shape[1])
    index.add(np.array(document_embeddings).astype(np.float32))  # FAISSì— ì„ë² ë”© ì¶”ê°€
    
    return index, documents, filenames

# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
index, documents, filenames = load_vector_store()

# GPT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
@st.cache_resource
def load_gpt_model():
    model = GPT2LMHeadModel.from_pretrained("distilgpt2")
    tokenizer = GPT2Tokenizer.from_pretrained("distilgpt2")
    return model, tokenizer

gpt_model, gpt_tokenizer = load_gpt_model()

# ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ì…ë ¥ì„ ì„ë² ë”©í•˜ê³  ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
    query_embedding = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2').encode([prompt])
    D, I = index.search(np.array(query_embedding).astype(np.float32), k=3)
    
    # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ GPT ëª¨ë¸ì—ì„œ ë‹µë³€ ìƒì„±
    context = " ".join([documents[i] for i in I[0]])
    
    # ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (GPT-2ì˜ ìµœëŒ€ ì…ë ¥ í† í°ì€ 1024 í† í°)
    max_context_length = 500  # í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ 500ìë¡œ ì œí•œ
    limited_context = context[:max_context_length]  # í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì œí•œ
    
    input_text = f"ë§¥ë½: {limited_context}\n\nì§ˆë¬¸: {prompt}\n\në‹µë³€:"
    input_ids = gpt_tokenizer.encode(input_text, return_tensors="pt")

    # max_new_tokensë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì´ ìƒì„±í•  í† í° ìˆ˜ë¥¼ ì œí•œ
    output = gpt_model.generate(input_ids, max_new_tokens=150)
    
    response_text = gpt_tokenizer.decode(output[0], skip_special_tokens=True)

    with st.chat_message("assistant"):
        st.markdown(response_text)
    
    st.session_state.messages.append({"role": "assistant", "content": response_text})
